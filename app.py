# -*- coding: utf-8 -*-
"""Summarizer App.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y0XqxB4KwkzzPdyoA3mZqT0bLLad_saE
"""

from transformers import T5ForConditionalGeneration, T5Tokenizer
import torch
import re
import random

def augment_input(text):
    """
    Slightly alters the input to introduce variation in generation.
    This could involve shuffling sentences, adding minor changes to the text.
    """
    sentences = text.split(". ")  # Split text into sentences based on periods
    random.shuffle(sentences)  # Shuffle sentence order slightly to introduce variability
    augmented_text = ". ".join(sentences).strip()  # Rejoin sentences
    return augmented_text

def abstractive_summarizer(text):
    """
    Summarizes the given input text using the T5 transformer model and generates new content
    while maintaining the same context. Each time, a single output is generated with randomness.

    Args:
        text (str): The input text to summarize.

    Returns:
        str: The generated summary, or an error message if input is invalid.
    """
    try:
        # Step 1: Check if input text is empty or contains only whitespace
        if not text.strip():
            return "Error: Input text is empty. Please provide valid text."

        # Step 2: Remove any unwanted characters while keeping punctuation
        cleaned_text = re.sub(r'[^A-Za-z0-9\s.,!?]', '', text).strip()

        # Ensure there is enough meaningful content
        if len(cleaned_text.split()) < 3:
            return "Error: Input text contains insufficient meaningful content. Please provide valid text."

        # Step 3: Augment the input slightly to ensure variability in generation
        augmented_text = augment_input(cleaned_text)

        # Step 4: Load the pre-trained T5 model and tokenizer
        model = T5ForConditionalGeneration.from_pretrained('t5-large')  # Using 't5-large' for better generation
        tokenizer = T5Tokenizer.from_pretrained('t5-large')

        # Step 5: Check if a GPU is available, otherwise use CPU
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        model = model.to(device)

        # Step 6: Prepend "summarize:" to the augmented text to tell the model to summarize
        input_text = "summarize: " + augmented_text

        # Step 7: Tokenize the input text for the model
        inputs = tokenizer.encode(input_text, return_tensors="pt", max_length=512, truncation=True)
        inputs = inputs.to(device)

        # Step 8: Generate a single summary with randomness
        summary_ids = model.generate(
            inputs,
            max_length=100,
            min_length=30,
            temperature=1.7,          # Increased temperature for more randomness
            top_k=50,                 # Top-k sampling to choose from top 50 most likely next words
            top_p=0.9,                # Nucleus sampling to consider top 90% probability mass
            no_repeat_ngram_size=3,   # Avoid repeating phrases
            repetition_penalty=2.0,   # Penalize repetition of the same words
            num_return_sequences=1,   # Only generate a single summary
            early_stopping=True
        )

        # Step 9: Decode the output summary back into human-readable text
        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

        return summary  # Return the generated summary

    except Exception as e:
        return f"An error occurred: {str(e)}"


if __name__ == "__main__":
    # Ask the user to input text to summarize
    user_input = input("Enter text to summarize: ")

    # Step 10: Check if input is valid
    if user_input.strip():
        summary = abstractive_summarizer(user_input)  # Generate a single summary
        print("\nSummary:\n", summary)  # Print the summary
    else:
        print("Error: Please provide valid input text for summarization.")

